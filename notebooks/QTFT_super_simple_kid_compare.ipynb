{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6668bcb9",
   "metadata": {},
   "source": [
    "# üéà Super Simple QTFT (Kid Edition) ‚Äî Side‚Äëby‚ÄëSide Forecasting (Polished)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b1ee6",
   "metadata": {},
   "source": [
    "**Goal:** Compare a tiny **Normal LSTM** vs a **Quantum‚Äëflavored LSTM (‚ÄúMagic Box‚Äù)** on a noisy time series.\n",
    "\n",
    "**What you‚Äôll see**\n",
    "- Train/test loss curves\n",
    "- A side‚Äëby‚Äëside forecast plot (true vs predictions)\n",
    "- Final test losses for both models\n",
    "\n",
    "**How to read results**\n",
    "- Lower test loss = better generalization\n",
    "- Prediction lines close to the blue **True** line = better forecasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bb3ad9",
   "metadata": {},
   "source": [
    "## 1) Switches (tweak and re‚Äërun)\n",
    "> Try: `EPOCHS=50`, `LR=0.02`, `NOISE=0.3`, `D_MODEL=24`, `LSTM_HIDDEN=12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda61d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAST_STEPS=10; FUTURE_STEPS=5; EPOCHS=30; LR=0.05; NOISE=0.20; N_SAMPLES=260; D_MODEL=16; LSTM_HIDDEN=8; PRINT_EVERY=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f049719",
   "metadata": {},
   "source": [
    "## 2) Tools we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb8877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "\n",
    "SEED=1234; random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d834c",
   "metadata": {},
   "source": [
    "## 3) Make a tiny pretend world üåç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0622ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic(n=260, noise=0.2):\n",
    "    t = np.arange(n)\n",
    "    base = 10 + 2*np.sin(2*np.pi*t/30.0) + 0.7*np.sin(2*np.pi*t/7.0)\n",
    "    y = base + noise*np.random.randn(n)\n",
    "    Open = y + 0.05*np.random.randn(n)\n",
    "    High = y + np.abs(0.2*np.random.randn(n))\n",
    "    Low  = y - np.abs(0.2*np.random.randn(n))\n",
    "    Last = y + 0.03*np.random.randn(n)\n",
    "    Close= y + 0.04*np.random.randn(n)\n",
    "    X = np.stack([Open, High, Low, Last], axis=1).astype(np.float32)\n",
    "    return X, Close.astype(np.float32)\n",
    "\n",
    "X, y = make_synthetic(N_SAMPLES, NOISE)\n",
    "plt.figure(); plt.plot(y, label='Close (target)')\n",
    "plt.title('Our pretend world: target over time'); plt.xlabel('time'); plt.ylabel('price')\n",
    "plt.legend(); plt.grid(True, alpha=0.2); plt.show()\n",
    "print('Features shape:', X.shape, '| Target shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec5c5e1",
   "metadata": {},
   "source": [
    "## 4) Slice into stories (past ‚Üí future) + scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210f6048",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_scale(arr):\n",
    "    mn, mx = arr.min(), arr.max()\n",
    "    if mx - mn < 1e-8: return arr*0, (mn, mx)\n",
    "    return (arr - mn) / (mx - mn), (mn, mx)\n",
    "def minmax_unscale(arr, mm):\n",
    "    mn, mx = mm; return arr*(mx-mn) + mn\n",
    "\n",
    "X_scaled = X.copy().astype(np.float32); mm_X = []\n",
    "for c in range(X.shape[1]):\n",
    "    X_scaled[:,c], mmc = minmax_scale(X[:,c]); mm_X.append(mmc)\n",
    "y_scaled, mm_y = minmax_scale(y.astype(np.float32))\n",
    "\n",
    "def build_windows(Xa, ya, past, future):\n",
    "    Xp, Xf, Y = [], [], []\n",
    "    for t in range(past, len(Xa)-future):\n",
    "        Xp.append(Xa[t-past:t])\n",
    "        Xf.append(Xa[t:t+future])\n",
    "        Y.append(ya[t:t+future])\n",
    "    Xp = np.stack(Xp).astype(np.float32)\n",
    "    Xf = np.stack(Xf).astype(np.float32)\n",
    "    Y  = np.stack(Y).astype(np.float32)[..., None]\n",
    "    return Xp, Xf, Y\n",
    "\n",
    "Xp, Xf, Y = build_windows(X_scaled, y_scaled, PAST_STEPS, FUTURE_STEPS)\n",
    "N = len(Xp); cut = int(0.7*N)\n",
    "Xptr, Xftr, Ytr = torch.tensor(Xp[:cut]).to(device), torch.tensor(Xf[:cut]).to(device), torch.tensor(Y[:cut]).to(device)\n",
    "Xpte, Xfte, Yte = torch.tensor(Xp[cut:]).to(device), torch.tensor(Xf[cut:]).to(device), torch.tensor(Y[cut:]).to(device)\n",
    "print('Train shapes:', Xptr.shape, Xftr.shape, Ytr.shape)\n",
    "print('Test  shapes:', Xpte.shape, Xfte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6d5345",
   "metadata": {},
   "source": [
    "## 5) Models at a glance\n",
    "\n",
    "**Normal Brain (LSTM):**\n",
    "- Projects features ‚Üí encodes past with LSTM ‚Üí decodes future with LSTM ‚Üí linear head.\n",
    "\n",
    "**Magic Box Brain (Quantum‚Äëflavored):**\n",
    "- Same backbone as Normal, but inserts a ‚ÄúMagic Box‚Äù:\n",
    "  - `sin/cos` encode ‚Üí tiny mixing with gates ‚Üí `tanh` readout\n",
    "- This mimics a quantum layer‚Äôs encode ‚Üí entangle ‚Üí measure vibe (implemented in PyTorch only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e9ff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalBrain(nn.Module):\n",
    "    def __init__(self, n_features=4, d_model=16, lstm_hidden=8, future=5):\n",
    "        super().__init__()\n",
    "        self.var_proj = nn.Linear(n_features, d_model)\n",
    "        self.enc = nn.LSTM(d_model, lstm_hidden, batch_first=True)\n",
    "        self.dec = nn.LSTM(d_model, lstm_hidden, batch_first=True)\n",
    "        self.up  = nn.Linear(lstm_hidden, d_model)\n",
    "        self.head= nn.Linear(d_model, 1)\n",
    "    def forward(self, Xp, Xf):\n",
    "        xp = self.var_proj(Xp); xf = self.var_proj(Xf)\n",
    "        _, (h, c) = self.enc(xp)\n",
    "        dec_out, _ = self.dec(xf, (h, c))\n",
    "        z  = self.up(dec_out)\n",
    "        return self.head(z)\n",
    "\n",
    "class MagicBox(nn.Module):\n",
    "    def __init__(self, d_model=16):\n",
    "        super().__init__()\n",
    "        self.enc  = nn.Linear(d_model*2, d_model)\n",
    "        self.mix1 = nn.Linear(d_model, d_model)\n",
    "        self.mix2 = nn.Linear(d_model, d_model)\n",
    "    def forward(self, x):\n",
    "        s, c = torch.sin(x), torch.cos(x)\n",
    "        h = torch.cat([s, c], dim=-1)\n",
    "        h = F.elu(self.enc(h))\n",
    "        g = torch.sigmoid(self.mix1(h))\n",
    "        m = F.elu(self.mix2(h))\n",
    "        z = g*m + (1-g)*h\n",
    "        return torch.tanh(z)\n",
    "\n",
    "class MagicBrain(nn.Module):\n",
    "    def __init__(self, n_features=4, d_model=16, lstm_hidden=8, future=5):\n",
    "        super().__init__()\n",
    "        self.var_proj = nn.Linear(n_features, d_model)\n",
    "        self.enc = nn.LSTM(d_model, lstm_hidden, batch_first=True)\n",
    "        self.dec = nn.LSTM(d_model, lstm_hidden, batch_first=True)\n",
    "        self.up  = nn.Linear(lstm_hidden, d_model)\n",
    "        self.magic1 = MagicBox(d_model)\n",
    "        self.magic2 = MagicBox(d_model)\n",
    "        self.head= nn.Linear(d_model, 1)\n",
    "    def forward(self, Xp, Xf):\n",
    "        xp = self.var_proj(Xp); xf = self.var_proj(Xf)\n",
    "        _, (h, c) = self.enc(xp)\n",
    "        dec_out, _ = self.dec(xf, (h, c))\n",
    "        z  = self.up(dec_out)\n",
    "        z  = self.magic1(z); z  = self.magic2(z)\n",
    "        return self.head(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ec2dfe",
   "metadata": {},
   "source": [
    "## 6) Train both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae_loss(y_true, y_pred): return torch.mean(torch.abs(y_true - y_pred))\n",
    "\n",
    "def train(model, Xptr, Xftr, Ytr, Xpte, Xfte, Yte, epochs=30, lr=0.05, print_every=5):\n",
    "    model.to(device); opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    tr, te = [], []\n",
    "    for ep in range(epochs):\n",
    "        model.train(); opt.zero_grad()\n",
    "        yhat = model(Xptr, Xftr); loss = mae_loss(Ytr, yhat)\n",
    "        loss.backward(); opt.step(); tr.append(loss.item())\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            yhat_te = model(Xpte, Xfte)\n",
    "            te.append(mae_loss(Yte, yhat_te).item())\n",
    "        if (ep+1) % print_every == 0 or ep == 0:\n",
    "            print(f\"Epoch {ep+1:3d} | train {tr[-1]:.4f} | test {te[-1]:.4f}\")\n",
    "    return tr, te, model\n",
    "\n",
    "torch.manual_seed(5678)\n",
    "c_model=NormalBrain(d_model=D_MODEL, lstm_hidden=LSTM_HIDDEN, future=FUTURE_STEPS).to(device)\n",
    "m_model=MagicBrain(d_model=D_MODEL, lstm_hidden=LSTM_HIDDEN, future=FUTURE_STEPS).to(device)\n",
    "tr_c, te_c, c_model = train(c_model, Xptr, Xftr, Ytr, Xpte, Xfte, Yte, epochs=EPOCHS, lr=LR, print_every=PRINT_EVERY)\n",
    "tr_m, te_m, m_model = train(m_model, Xptr, Xftr, Ytr, Xpte, Xfte, Yte, epochs=EPOCHS, lr=LR, print_every=PRINT_EVERY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58dc58b",
   "metadata": {},
   "source": [
    "## 7) Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f2fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(te_c, label='Normal LSTM ‚Äî test loss')\n",
    "plt.plot(te_m, label='Magic Box LSTM ‚Äî test loss')\n",
    "plt.title('Test Loss per Epoch (lower is better)')\n",
    "plt.xlabel('Epoch'); plt.ylabel('MAE'); plt.legend(); plt.grid(True, alpha=0.2); plt.show()\n",
    "print(f\"Final test loss ‚Äî Normal LSTM: {te_c[-1]:.4f}\")\n",
    "print(f\"Final test loss ‚Äî Magic Box   : {te_m[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38d3363",
   "metadata": {},
   "source": [
    "## 8) One forecast horizon (true vs predictions) + save preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9cc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    yhat_c = c_model(Xpte[:1], Xfte[:1]).cpu().numpy()[0,:,0]\n",
    "    yhat_m = m_model(Xpte[:1], Xfte[:1]).cpu().numpy()[0,:,0]\n",
    "true = Yte[:1].cpu().numpy()[0,:,0]\n",
    "\n",
    "def unscale_y(a): return minmax_unscale(a, mm_y)\n",
    "\n",
    "true_i = unscale_y(true); pred_c = unscale_y(yhat_c); pred_m = unscale_y(yhat_m)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_i, label='True', linewidth=2)\n",
    "plt.plot(pred_c, '--', label='Normal LSTM')\n",
    "plt.plot(pred_m, ':',  label='Magic Box LSTM')\n",
    "plt.title('One Forecast Horizon (True vs Predictions)'); plt.xlabel('Future step'); plt.ylabel('Price')\n",
    "plt.legend(); plt.grid(True, alpha=0.2); plt.tight_layout(); plt.show()\n",
    "\n",
    "best = \"Normal LSTM\" if te_c[-1] <= te_m[-1] else \"Magic Box LSTM\"\n",
    "print(f\"‚úÖ Quick verdict on this run: {best} achieved the lower final test loss.\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(true_i, label='True', linewidth=2)\n",
    "plt.plot(pred_c, '--', label='Normal LSTM')\n",
    "plt.plot(pred_m, ':',  label='Magic Box LSTM')\n",
    "plt.title('One Forecast Horizon (True vs Predictions)'); plt.xlabel('Future step'); plt.ylabel('Price')\n",
    "plt.legend(); plt.grid(True, alpha=0.2); plt.tight_layout()\n",
    "plt.savefig('preview.png', dpi=180)\n",
    "print(\"Saved preview.png (upload to imgs/preview.png in your repo).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de965d1",
   "metadata": {},
   "source": [
    "## 9) Results summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577326d1",
   "metadata": {},
   "source": [
    "- **Final test loss (lower is better)** ‚Äî printed above for each model.\n",
    "- In the forecast plot, the best model‚Äôs line should follow the blue **True** line more closely.\n",
    "\n",
    "> Because the data is noisy and the models are tiny, results can vary. Try: more epochs (50‚Äì100), lower LR (0.02), or bigger models (`D_MODEL=24`, `LSTM_HIDDEN=12`)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
